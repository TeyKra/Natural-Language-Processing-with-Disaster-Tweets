{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":17777,"databundleVersionId":869809,"sourceType":"competition"}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Install the Required Libraries:","metadata":{}},{"cell_type":"code","source":"! pip install pandas numpy torch optuna tqdm transformers scikit-learn\n","metadata":{"execution":{"iopub.status.busy":"2025-01-06T13:31:41.229195Z","iopub.execute_input":"2025-01-06T13:31:41.229842Z","iopub.status.idle":"2025-01-06T13:31:50.580718Z","shell.execute_reply.started":"2025-01-06T13:31:41.229809Z","shell.execute_reply":"2025-01-06T13:31:50.579899Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (2.2.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (1.26.4)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.4.0)\nRequirement already satisfied: optuna in /opt/conda/lib/python3.10/site-packages (4.0.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (4.66.4)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.45.1)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (1.2.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas) (2024.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.15.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch) (2024.6.1)\nRequirement already satisfied: alembic>=1.5.0 in /opt/conda/lib/python3.10/site-packages (from optuna) (1.13.3)\nRequirement already satisfied: colorlog in /opt/conda/lib/python3.10/site-packages (from optuna) (6.8.2)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from optuna) (21.3)\nRequirement already satisfied: sqlalchemy>=1.3.0 in /opt/conda/lib/python3.10/site-packages (from optuna) (2.0.30)\nRequirement already satisfied: PyYAML in /opt/conda/lib/python3.10/site-packages (from optuna) (6.0.2)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.25.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.5)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.20.0)\nRequirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.14.1)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (3.5.0)\nRequirement already satisfied: Mako in /opt/conda/lib/python3.10/site-packages (from alembic>=1.5.0->optuna) (1.3.5)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->optuna) (3.1.2)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\nRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from sqlalchemy>=1.3.0->optuna) (3.0.3)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.8.30)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"### Import Libraries:","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nimport optuna\nimport random\nfrom tqdm import tqdm\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\nfrom transformers import BertTokenizer, BertForSequenceClassification, get_linear_schedule_with_warmup, get_scheduler\nfrom sklearn.model_selection import train_test_split\nfrom torch.optim import AdamW\nfrom sklearn.metrics import f1_score\nimport time\nimport datetime\n","metadata":{"execution":{"iopub.status.busy":"2025-01-06T13:32:03.567205Z","iopub.execute_input":"2025-01-06T13:32:03.568060Z","iopub.status.idle":"2025-01-06T13:32:09.843430Z","shell.execute_reply.started":"2025-01-06T13:32:03.568023Z","shell.execute_reply":"2025-01-06T13:32:09.842775Z"},"trusted":true},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"### Configure the device:","metadata":{}},{"cell_type":"code","source":"# Check if a CUDA-capable GPU is available and set the device accordingly.\n# If a GPU is available, it will use \"cuda\"; otherwise, it defaults to \"cpu.\"\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Print the device being used (either \"cuda\" for GPU or \"cpu\").\nprint(f\"Using device: {device}\")\n","metadata":{"execution":{"iopub.status.busy":"2025-01-06T13:32:14.647712Z","iopub.execute_input":"2025-01-06T13:32:14.648194Z","iopub.status.idle":"2025-01-06T13:32:14.762576Z","shell.execute_reply.started":"2025-01-06T13:32:14.648163Z","shell.execute_reply":"2025-01-06T13:32:14.761157Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"### Load and prepare data:","metadata":{}},{"cell_type":"code","source":"# Load the data\n# Read the training and testing datasets from their respective file paths.\ntrain = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\ntest = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')\n\n# Fill missing values\n# Replace missing values in the 'keyword' column with an empty string.\ntrain['keyword'] = train['keyword'].fillna('')\ntest['keyword'] = test['keyword'].fillna('')\n\n# Combine 'keyword' and 'text'\n# Create a new column 'text_combined' by concatenating 'keyword' and 'text' with a space in between.\ntrain['text_combined'] = train['keyword'] + ' ' + train['text']\ntest['text_combined'] = test['keyword'] + ' ' + test['text']\n","metadata":{"execution":{"iopub.status.busy":"2025-01-06T13:32:18.861656Z","iopub.execute_input":"2025-01-06T13:32:18.862188Z","iopub.status.idle":"2025-01-06T13:32:18.936838Z","shell.execute_reply.started":"2025-01-06T13:32:18.862158Z","shell.execute_reply":"2025-01-06T13:32:18.935993Z"},"trusted":true},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"### Preprocess texts with the BERT tokenizer:","metadata":{}},{"cell_type":"code","source":"# Load the BERT tokenizer\n# Initialize the tokenizer from the pre-trained BERT model ('bert-base-uncased'),\n# specifying that all text will be converted to lowercase.\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n\n# Define the maximum sequence length\n# Sequences longer than this value will be truncated, and shorter ones will be padded.\nMAX_LEN = 128\n\n# Tokenize the training data\n# Prepare the input IDs and attention masks for the training dataset.\ninput_ids = []\nattention_masks = []\n\nfor text in train['text_combined']:\n    # Encode each text using the BERT tokenizer\n    encoded_dict = tokenizer.encode_plus(\n                        text,                      # The input text to tokenize\n                        add_special_tokens=True,   # Add '[CLS]' and '[SEP]' tokens\n                        max_length=MAX_LEN,        # Pad or truncate to this length\n                        padding='max_length',      # Pad sequences to the maximum length\n                        truncation=True,           # Truncate sequences longer than MAX_LEN\n                        return_attention_mask=True,# Generate the attention mask\n                        return_tensors='pt',       # Return PyTorch tensors\n                   )\n    # Append the generated input IDs and attention masks\n    input_ids.append(encoded_dict['input_ids'])\n    attention_masks.append(encoded_dict['attention_mask'])\n\n# Convert lists to tensors\n# Stack all input IDs and attention masks into tensors for use in the model.\ninput_ids = torch.cat(input_ids, dim=0)\nattention_masks = torch.cat(attention_masks, dim=0)\nlabels = torch.tensor(train['target'].values)  # Convert target labels to a tensor\n","metadata":{"execution":{"iopub.status.busy":"2025-01-06T13:32:36.670723Z","iopub.execute_input":"2025-01-06T13:32:36.671058Z","iopub.status.idle":"2025-01-06T13:32:41.575438Z","shell.execute_reply.started":"2025-01-06T13:32:36.671030Z","shell.execute_reply":"2025-01-06T13:32:41.574660Z"},"trusted":true},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"### Split the data into training and validation sets:","metadata":{}},{"cell_type":"code","source":"# Split the dataset into training and validation sets\n# The input IDs and labels are split into training and validation subsets.\ntrain_inputs, validation_inputs, train_labels, validation_labels = train_test_split(\n    input_ids,              # The tokenized input IDs\n    labels,                 # The target labels\n    random_state=42,        # Seed for reproducibility\n    test_size=0.1           # 10% of the data will be used for validation\n)\n\n# Split the attention masks into training and validation sets\n# Only the attention masks are split here, corresponding to the input IDs.\ntrain_masks, validation_masks, _, _ = train_test_split(\n    attention_masks,        # The attention masks\n    input_ids,              # Dummy variable for alignment; not used here\n    random_state=42,        # Seed for reproducibility\n    test_size=0.1           # 10% of the masks will be used for validation\n)\n","metadata":{"execution":{"iopub.status.busy":"2025-01-06T13:32:48.445159Z","iopub.execute_input":"2025-01-06T13:32:48.445505Z","iopub.status.idle":"2025-01-06T13:32:48.468831Z","shell.execute_reply.started":"2025-01-06T13:32:48.445453Z","shell.execute_reply":"2025-01-06T13:32:48.467391Z"},"trusted":true},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"### Create the DataLoaders:","metadata":{}},{"cell_type":"code","source":"# Define the batch size for data loading\nbatch_size = 16\n\n# Create the training dataset\n# Combine input IDs, attention masks, and labels into a single dataset.\ntrain_data = TensorDataset(train_inputs, train_masks, train_labels)\n\n# Create a sampler for the training data\n# Randomly sample data to shuffle the dataset during training.\ntrain_sampler = RandomSampler(train_data)\n\n# Create a DataLoader for the training dataset\n# The DataLoader loads the data in batches of the specified size (16).\ntrain_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n\n# Create the validation dataset\n# Combine validation input IDs, attention masks, and labels into a single dataset.\nvalidation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n\n# Create a sequential sampler for the validation data\n# Sequential sampling is used to maintain the order of the validation data.\nvalidation_sampler = SequentialSampler(validation_data)\n\n# Create a DataLoader for the validation dataset\n# The DataLoader loads the validation data in batches of the specified size (16).\nvalidation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n","metadata":{"execution":{"iopub.status.busy":"2025-01-06T13:32:58.754305Z","iopub.execute_input":"2025-01-06T13:32:58.754903Z","iopub.status.idle":"2025-01-06T13:32:58.760072Z","shell.execute_reply.started":"2025-01-06T13:32:58.754870Z","shell.execute_reply":"2025-01-06T13:32:58.759002Z"},"trusted":true},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"### Load the pre-trained BERT model:","metadata":{}},{"cell_type":"code","source":"# Load the pre-trained BERT model for sequence classification\n# Initialize the BERT model ('bert-base-uncased') for a binary classification task.\nmodel = BertForSequenceClassification.from_pretrained(\n    'bert-base-uncased',       # The pre-trained BERT model to load\n    num_labels=2,              # Number of output labels (binary classification: 0 or 1)\n    output_attentions=False,   # Do not return attention weights\n    output_hidden_states=False # Do not return hidden states\n)\n\n# Move the model to the specified device (GPU or CPU)\nmodel.to(device)\n","metadata":{"execution":{"iopub.status.busy":"2025-01-06T13:33:01.468206Z","iopub.execute_input":"2025-01-06T13:33:01.469129Z","iopub.status.idle":"2025-01-06T13:33:04.623168Z","shell.execute_reply.started":"2025-01-06T13:33:01.469081Z","shell.execute_reply":"2025-01-06T13:33:04.622292Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f3603c5f634b4a4ba9a3aa768661a97b"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"BertForSequenceClassification(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x BertLayer(\n          (attention): BertAttention(\n            (self): BertSdpaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (classifier): Linear(in_features=768, out_features=2, bias=True)\n)"},"metadata":{}}],"execution_count":12},{"cell_type":"markdown","source":"### Configure the optimizer and scheduler:","metadata":{}},{"cell_type":"code","source":"# Initialize the AdamW optimizer\n# This optimizer implements the Adam algorithm with weight decay, as recommended for training BERT.\noptimizer = AdamW(\n    model.parameters(), # The model's parameters to be optimized\n    lr=2e-5,            # Learning rate: a small value for fine-tuning BERT\n    eps=1e-8            # Epsilon: prevents division by zero in the Adam algorithm\n)\n\n# Define the number of training epochs\nepochs = 10  # Number of complete passes through the training dataset\n\n# Calculate the total number of training steps\n# This is the number of batches in an epoch multiplied by the number of epochs.\ntotal_steps = len(train_dataloader) * epochs\n\n# Set up a learning rate scheduler\n# The scheduler linearly decreases the learning rate over the training steps.\nscheduler = get_scheduler(\n    \"linear\",               # Type of scheduler (linear decay of learning rate)\n    optimizer=optimizer,    # The optimizer whose learning rate is to be scheduled\n    num_warmup_steps=0,     # No warm-up steps; learning rate starts decreasing from the beginning\n    num_training_steps=total_steps # Total number of training steps\n)\n","metadata":{"execution":{"iopub.status.busy":"2025-01-06T13:33:10.247669Z","iopub.execute_input":"2025-01-06T13:33:10.247993Z","iopub.status.idle":"2025-01-06T13:33:10.700107Z","shell.execute_reply.started":"2025-01-06T13:33:10.247967Z","shell.execute_reply":"2025-01-06T13:33:10.699454Z"},"trusted":true},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"### Hyperparameter search:","metadata":{}},{"cell_type":"code","source":"# Define the objective function for hyperparameter tuning\ndef objective(trial):\n    # Define the hyperparameters to search\n    lr = trial.suggest_float(\"lr\", 1e-5, 5e-5, log=True)  # Learning rate\n    eps = trial.suggest_float(\"eps\", 1e-8, 1e-6, log=True)  # Epsilon for AdamW\n    batch_size = trial.suggest_categorical(\"batch_size\", [8, 16, 32])  # Batch sizes\n    epochs = trial.suggest_int(\"epochs\", 3, 10)  # Number of epochs\n\n    # Prepare the DataLoaders with the selected batch size\n    train_dataloader = DataLoader(\n        train_data, sampler=RandomSampler(train_data), batch_size=batch_size\n    )\n    validation_dataloader = DataLoader(\n        validation_data, sampler=SequentialSampler(validation_data), batch_size=batch_size\n    )\n\n    # Load the pre-trained BERT model\n    model = BertForSequenceClassification.from_pretrained(\n        \"bert-base-uncased\",\n        num_labels=2,              # Binary classification\n        output_attentions=False,   # Do not return attention weights\n        output_hidden_states=False # Do not return hidden states\n    )\n    model.to(device)  # Move the model to the selected device\n\n    # Configure the optimizer and scheduler\n    optimizer = AdamW(model.parameters(), lr=lr, eps=eps)\n    total_steps = len(train_dataloader) * epochs\n    scheduler = get_scheduler(\n        \"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=total_steps\n    )\n\n    # Train the model\n    best_f1 = 0  # Track the best F1 score\n    for epoch in range(epochs):\n        model.train()  # Set the model to training mode\n        total_loss = 0\n\n        for batch in tqdm(train_dataloader, desc=f\"Training Epoch {epoch + 1}\"):\n            b_input_ids = batch[0].to(device)\n            b_input_mask = batch[1].to(device)\n            b_labels = batch[2].to(device)\n\n            model.zero_grad()  # Clear the gradients\n            outputs = model(b_input_ids, attention_mask=b_input_mask, labels=b_labels)\n            loss = outputs.loss\n            total_loss += loss.item()\n\n            loss.backward()  # Backpropagate the loss\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # Gradient clipping\n            optimizer.step()  # Update weights\n            scheduler.step()  # Update learning rate\n\n        # Validate the model\n        model.eval()  # Set the model to evaluation mode\n        predictions, true_labels = [], []\n        with torch.no_grad():\n            for batch in validation_dataloader:\n                b_input_ids = batch[0].to(device)\n                b_input_mask = batch[1].to(device)\n                b_labels = batch[2].to(device)\n\n                outputs = model(b_input_ids, attention_mask=b_input_mask)\n                logits = outputs.logits\n                predictions.append(logits.detach().cpu().numpy())\n                true_labels.append(b_labels.cpu().numpy())\n\n        # Flatten predictions and true labels\n        flat_predictions = np.concatenate(predictions, axis=0)\n        flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n        flat_true_labels = np.concatenate(true_labels, axis=0)\n\n        # Calculate the F1 score\n        f1 = f1_score(flat_true_labels, flat_predictions)\n\n        # Track the best F1 score\n        if f1 > best_f1:\n            best_f1 = f1\n\n    # Return the best F1 score\n    return best_f1\n\n\n# Define and run the study\nstudy = optuna.create_study(direction=\"maximize\")  # Aim to maximize the F1 score\nstudy.optimize(objective, n_trials=1)  # Run 20 trials to find the best hyperparameters\n\n# Display the best hyperparameters and F1 score\nprint(\"\\nBest hyperparameters:\", study.best_params)\nprint(\"Best F1 Score:\", study.best_value)\n","metadata":{"execution":{"iopub.status.busy":"2025-01-06T13:45:50.213414Z","iopub.execute_input":"2025-01-06T13:45:50.214054Z","iopub.status.idle":"2025-01-06T13:53:49.305950Z","shell.execute_reply.started":"2025-01-06T13:45:50.214023Z","shell.execute_reply":"2025-01-06T13:53:49.305161Z"},"trusted":true},"outputs":[{"name":"stderr","text":"[I 2025-01-06 13:45:50,222] A new study created in memory with name: no-name-db50f922-3550-4332-bc80-8ea7f83beca9\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nTraining Epoch 1: 100%|██████████| 429/429 [02:34<00:00,  2.79it/s]\nTraining Epoch 2: 100%|██████████| 429/429 [02:33<00:00,  2.79it/s]\nTraining Epoch 3: 100%|██████████| 429/429 [02:33<00:00,  2.80it/s]\n[I 2025-01-06 13:53:49,301] Trial 0 finished with value: 0.8005997001499251 and parameters: {'lr': 4.465530103671243e-05, 'eps': 4.3427525694836487e-07, 'batch_size': 16, 'epochs': 3}. Best is trial 0 with value: 0.8005997001499251.\n","output_type":"stream"},{"name":"stdout","text":"\nBest hyperparameters: {'lr': 4.465530103671243e-05, 'eps': 4.3427525694836487e-07, 'batch_size': 16, 'epochs': 3}\nBest F1 Score: 0.8005997001499251\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"### Train the model:","metadata":{}},{"cell_type":"code","source":"# Seed for reproducibility\n# Set random seeds for consistent results across runs.\nseed_val = 42\nrandom.seed(seed_val)\nnp.random.seed(seed_val)\ntorch.manual_seed(seed_val)\ntorch.cuda.manual_seed_all(seed_val)\n\n# Training and validation loop with callbacks\nbest_f1 = 0  # Track the best F1 score\nearly_stopping_patience = 3  # Maximum number of epochs to wait for improvement\npatience_counter = 0  # Counter for early stopping patience\n\nfor epoch_i in range(epochs):\n    print(f\"\\n======== Epoch {epoch_i + 1}/{epochs} ========\")\n\n    # Training phase\n    print(\"Training...\")\n    model.train()\n    total_loss = 0\n\n    # Progress bar for batches\n    progress_bar = tqdm(train_dataloader, desc=\"Batch\")\n    for step, batch in enumerate(progress_bar):\n        b_input_ids = batch[0].to(device)\n        b_input_mask = batch[1].to(device)\n        b_labels = batch[2].to(device)\n\n        model.zero_grad()  # Clear gradients\n        outputs = model(b_input_ids, attention_mask=b_input_mask, labels=b_labels)\n        loss = outputs.loss\n\n        total_loss += loss.item()\n        loss.backward()  # Backpropagate the loss\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # Gradient clipping\n        optimizer.step()  # Update model weights\n        scheduler.step()  # Update learning rate\n\n        progress_bar.set_postfix({\"Batch Loss\": loss.item()})\n\n    # Calculate average training loss\n    avg_train_loss = total_loss / len(train_dataloader)\n    print(f\"\\nAverage training loss: {avg_train_loss:.2f}\")\n\n    # Validation phase\n    print(\"\\nValidation...\")\n    model.eval()\n    eval_loss, eval_accuracy = 0, 0\n    predictions, true_labels = [], []\n\n    with torch.no_grad():\n        for batch in tqdm(validation_dataloader, desc=\"Validation Batches\"):\n            b_input_ids = batch[0].to(device)\n            b_input_mask = batch[1].to(device)\n            b_labels = batch[2].to(device)\n\n            outputs = model(b_input_ids, attention_mask=b_input_mask)\n            logits = outputs.logits\n\n            logits = logits.detach().cpu().numpy()\n            label_ids = b_labels.cpu().numpy()\n\n            predictions.append(logits)\n            true_labels.append(label_ids)\n\n    # Calculate metrics\n    flat_predictions = np.concatenate(predictions, axis=0)\n    flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n    flat_true_labels = np.concatenate(true_labels, axis=0)\n\n    f1 = f1_score(flat_true_labels, flat_predictions)\n    print(f\"F1 Score: {f1:.2f}\")\n\n    # Early Stopping based on F1 Score\n    if f1 > best_f1:\n        best_f1 = f1\n        patience_counter = 0\n        torch.save(model.state_dict(), \"best_model.pt\")  # Save the best model\n        print(\"New best model saved.\")\n    else:\n        patience_counter += 1\n        print(f\"Early stopping patience: {patience_counter}/{early_stopping_patience}\")\n        if patience_counter >= early_stopping_patience:\n            print(\"Early stopping activated.\")\n            break\n\nprint(\"\\nTraining complete!\")\n","metadata":{"execution":{"iopub.status.busy":"2025-01-06T13:55:42.662061Z","iopub.execute_input":"2025-01-06T13:55:42.662724Z","iopub.status.idle":"2025-01-06T14:17:05.906443Z","shell.execute_reply.started":"2025-01-06T13:55:42.662693Z","shell.execute_reply":"2025-01-06T14:17:05.905511Z"},"trusted":true},"outputs":[{"name":"stdout","text":"\n======== Epoch 1/10 ========\nTraining...\n","output_type":"stream"},{"name":"stderr","text":"Batch: 100%|██████████| 429/429 [02:34<00:00,  2.77it/s, Batch Loss=1.38]  \n","output_type":"stream"},{"name":"stdout","text":"\nAverage training loss: 0.46\n\nValidation...\n","output_type":"stream"},{"name":"stderr","text":"Validation Batches: 100%|██████████| 48/48 [00:05<00:00,  8.34it/s]\n","output_type":"stream"},{"name":"stdout","text":"F1 Score: 0.77\nNew best model saved.\n\n======== Epoch 2/10 ========\nTraining...\n","output_type":"stream"},{"name":"stderr","text":"Batch: 100%|██████████| 429/429 [02:34<00:00,  2.78it/s, Batch Loss=0.0397]\n","output_type":"stream"},{"name":"stdout","text":"\nAverage training loss: 0.34\n\nValidation...\n","output_type":"stream"},{"name":"stderr","text":"Validation Batches: 100%|██████████| 48/48 [00:05<00:00,  8.34it/s]\n","output_type":"stream"},{"name":"stdout","text":"F1 Score: 0.79\nNew best model saved.\n\n======== Epoch 3/10 ========\nTraining...\n","output_type":"stream"},{"name":"stderr","text":"Batch: 100%|██████████| 429/429 [02:34<00:00,  2.78it/s, Batch Loss=0.0892]\n","output_type":"stream"},{"name":"stdout","text":"\nAverage training loss: 0.24\n\nValidation...\n","output_type":"stream"},{"name":"stderr","text":"Validation Batches: 100%|██████████| 48/48 [00:05<00:00,  8.34it/s]\n","output_type":"stream"},{"name":"stdout","text":"F1 Score: 0.80\nNew best model saved.\n\n======== Epoch 4/10 ========\nTraining...\n","output_type":"stream"},{"name":"stderr","text":"Batch: 100%|██████████| 429/429 [02:33<00:00,  2.79it/s, Batch Loss=0.0187] \n","output_type":"stream"},{"name":"stdout","text":"\nAverage training loss: 0.17\n\nValidation...\n","output_type":"stream"},{"name":"stderr","text":"Validation Batches: 100%|██████████| 48/48 [00:05<00:00,  8.34it/s]\n","output_type":"stream"},{"name":"stdout","text":"F1 Score: 0.79\nEarly stopping patience: 1/3\n\n======== Epoch 5/10 ========\nTraining...\n","output_type":"stream"},{"name":"stderr","text":"Batch: 100%|██████████| 429/429 [02:34<00:00,  2.78it/s, Batch Loss=0.00316] \n","output_type":"stream"},{"name":"stdout","text":"\nAverage training loss: 0.12\n\nValidation...\n","output_type":"stream"},{"name":"stderr","text":"Validation Batches: 100%|██████████| 48/48 [00:05<00:00,  8.35it/s]\n","output_type":"stream"},{"name":"stdout","text":"F1 Score: 0.80\nNew best model saved.\n\n======== Epoch 6/10 ========\nTraining...\n","output_type":"stream"},{"name":"stderr","text":"Batch: 100%|██████████| 429/429 [02:33<00:00,  2.79it/s, Batch Loss=0.000935]\n","output_type":"stream"},{"name":"stdout","text":"\nAverage training loss: 0.09\n\nValidation...\n","output_type":"stream"},{"name":"stderr","text":"Validation Batches: 100%|██████████| 48/48 [00:05<00:00,  8.34it/s]\n","output_type":"stream"},{"name":"stdout","text":"F1 Score: 0.80\nEarly stopping patience: 1/3\n\n======== Epoch 7/10 ========\nTraining...\n","output_type":"stream"},{"name":"stderr","text":"Batch: 100%|██████████| 429/429 [02:34<00:00,  2.78it/s, Batch Loss=0.0042]  \n","output_type":"stream"},{"name":"stdout","text":"\nAverage training loss: 0.07\n\nValidation...\n","output_type":"stream"},{"name":"stderr","text":"Validation Batches: 100%|██████████| 48/48 [00:05<00:00,  8.34it/s]\n","output_type":"stream"},{"name":"stdout","text":"F1 Score: 0.79\nEarly stopping patience: 2/3\n\n======== Epoch 8/10 ========\nTraining...\n","output_type":"stream"},{"name":"stderr","text":"Batch: 100%|██████████| 429/429 [02:34<00:00,  2.78it/s, Batch Loss=0.000228]\n","output_type":"stream"},{"name":"stdout","text":"\nAverage training loss: 0.05\n\nValidation...\n","output_type":"stream"},{"name":"stderr","text":"Validation Batches: 100%|██████████| 48/48 [00:05<00:00,  8.34it/s]","output_type":"stream"},{"name":"stdout","text":"F1 Score: 0.80\nEarly stopping patience: 3/3\nEarly stopping activated.\n\nTraining complete!\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":16},{"cell_type":"markdown","source":"### Predict on the test set:","metadata":{}},{"cell_type":"code","source":"print(\"Predicting on the test set...\")\n\n# Tokenize the test data\ntest_input_ids = []\ntest_attention_masks = []\n\nfor text in test['text_combined']:\n    # Encode each text sample using the tokenizer\n    encoded_dict = tokenizer.encode_plus(\n                        text,                      # Input text to tokenize\n                        add_special_tokens=True,   # Add '[CLS]' and '[SEP]' tokens\n                        max_length=MAX_LEN,        # Pad or truncate to this length\n                        padding='max_length',      # Pad sequences to the maximum length\n                        truncation=True,           # Truncate sequences longer than MAX_LEN\n                        return_attention_mask=True,# Generate the attention mask\n                        return_tensors='pt',       # Return PyTorch tensors\n                   )\n    test_input_ids.append(encoded_dict['input_ids'])\n    test_attention_masks.append(encoded_dict['attention_mask'])\n\n# Convert lists to tensors\n# Stack the input IDs and attention masks into tensors for the test set.\ntest_input_ids = torch.cat(test_input_ids, dim=0)\ntest_attention_masks = torch.cat(test_attention_masks, dim=0)\n\n# Create the DataLoader for the test set\n# Combine the input IDs and attention masks into a single dataset and load it in batches.\ntest_dataset = TensorDataset(test_input_ids, test_attention_masks)\ntest_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n\n# Prediction\nmodel.eval()  # Set the model to evaluation mode\npredictions = []\n\nfor batch in test_dataloader:\n    # Move the batch to the appropriate device (GPU or CPU)\n    batch = tuple(t.to(device) for t in batch)\n    b_input_ids, b_input_mask = batch\n\n    # Perform inference without gradient computation\n    with torch.no_grad():\n        outputs = model(b_input_ids, attention_mask=b_input_mask)\n\n    logits = outputs.logits  # Retrieve the raw output scores\n    logits = logits.detach().cpu().numpy()  # Move logits to the CPU and convert to NumPy\n    predictions.append(logits)\n\n# Flatten predictions\n# Concatenate all predictions into a single array and take the class with the highest score.\nflat_predictions = np.concatenate(predictions, axis=0)\nflat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n","metadata":{"execution":{"iopub.status.busy":"2025-01-06T14:18:45.530201Z","iopub.execute_input":"2025-01-06T14:18:45.530524Z","iopub.status.idle":"2025-01-06T14:19:13.245594Z","shell.execute_reply.started":"2025-01-06T14:18:45.530488Z","shell.execute_reply":"2025-01-06T14:19:13.244666Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Predicting on the test set...\n","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":"### Create the submission file:","metadata":{}},{"cell_type":"code","source":"# Create a submission DataFrame\n# Combine the test set IDs and the predicted target labels into a single DataFrame.\nsubmission = pd.DataFrame({'id': test['id'], 'target': flat_predictions})\n\n# Save the DataFrame to a CSV file\n# The file is named 'submission.csv', and the index is not included in the CSV file.\nsubmission.to_csv('submission.csv', index=False)\n\n# Confirmation message\nprint(\"The submission file has been successfully created.\")\n","metadata":{"execution":{"iopub.status.busy":"2025-01-06T14:19:37.808372Z","iopub.execute_input":"2025-01-06T14:19:37.808713Z","iopub.status.idle":"2025-01-06T14:19:37.821412Z","shell.execute_reply.started":"2025-01-06T14:19:37.808682Z","shell.execute_reply":"2025-01-06T14:19:37.820684Z"},"trusted":true},"outputs":[{"name":"stdout","text":"The submission file has been successfully created.\n","output_type":"stream"}],"execution_count":18}]}